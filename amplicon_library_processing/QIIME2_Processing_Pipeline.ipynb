{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Pipeline to Process Raw Sequences into Phyloseq Object with DADA2 ###\n",
    "* Prep for Import to QIIME2  (Combine two index files)\n",
    "* Import to QIIME2\n",
    "* Demultiplex\n",
    "* Denoise and Merge\n",
    "* Prepare OTU Tables and Rep Sequences  *(Note: sample names starting with a digit will break this step)*\n",
    "* Classify Seqs\n",
    "\n",
    "*100% Appropriated from the \"Atacama Desert Tutorial\" for QIIME2*\n",
    "\n",
    "### Pipeline can handle both 16S rRNA gene and ITS sequences (in theory)####\n",
    "* Tested on 515f and 806r\n",
    "* Tested on ITS1\n",
    "\n",
    "### Commands to Install Dependencies ####\n",
    "##### || QIIME2 ||\n",
    "** Note: QIIME2 is still actively in development, and I've noticed frequent new releases. Check for the most up-to-date conda install file <https://docs.qiime2.org/2017.11/install/native/#install-qiime-2-within-a-conda-environment>\n",
    "\n",
    "* wget https://data.qiime2.org/distro/core/qiime2-2018.2-py35-linux-conda.yml\n",
    "* conda env create -n qiime2-2018.2 --file qiime2-2018.2-py35-linux-conda.yml\n",
    "* source activate qiime2-pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### || Copyrighter rrn Database ||\n",
    "* The script will automatically install the curated GreenGenes rrn attribute database\n",
    "* https://github.com/fangly/AmpliCopyrighter\n",
    "\n",
    "##### || rpy2 (don't use conda version) ||\n",
    "* pip install rpy2  \n",
    "\n",
    "##### || phyloseq ||\n",
    "* conda install -c r r-igraph \n",
    "* Rscript -e \"source('http://bioconductor.org/biocLite.R');biocLite('phyloseq')\" \n",
    "\n",
    "##### || R packages ||\n",
    "* ape   (natively installed in conda environment)\n",
    "\n",
    "\n",
    "### Citations ###\n",
    "* Caporaso, J. G., Kuczynski, J., Stombaugh, J., Bittinger, K., Bushman, F. D., Costello, E. K., *et al.* (2010). QIIME allows analysis of high-throughput community sequencing data. Nature methods, 7(5), 335-336.\n",
    "\n",
    "\n",
    "* McMurdie and Holmes (2013) phyloseq: An R Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data. PLoS ONE. 8(4):e61217\n",
    "\n",
    "\n",
    "* Paradis E., Claude J. & Strimmer K. 2004. APE: analyses of phylogenetics and evolution in R language. Bioinformatics 20: 289-290.\n",
    "\n",
    "\n",
    "* Angly, F. E., Dennis, P. G., Skarshewski, A., Vanwonterghem, I., Hugenholtz, P., & Tyson, G. W. (2014). CopyRighter: a rapid tool for improving the accuracy of microbial community profiles through lineage-specific gene copy number correction. Microbiome, 2(1), 11.\n",
    "\n",
    "###### Last Modified by R. Wilhelm on October 12th, 2017 ######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, numpy as np\n",
    "\n",
    "# Provide the directory for your index and read files (you can do multiple independently in one go)\n",
    "bioblitz = '/home/roli/BioBlitz.2017/SV_based/'\n",
    "\n",
    "# Prepare an object with the name of the library, the name of the directory object (created above), and the metadatafile name\n",
    "#datasets = [['name',directory1,'metadata1','domain of life'],['name',directory2,'metadata2','domain of life']]\n",
    "datasets = [['bioblitz',bioblitz,'metadata.tsv','bacteria']]\n",
    "\n",
    "# Ensure your reads files are named accordingly (or modify to suit your needs)\n",
    "readFile1 = 'read1.fq.gz'\n",
    "readFile2 = 'read2.fq.gz'\n",
    "indexFile1 = 'index_read1.fq.gz'\n",
    "indexFile2 = 'index_read2.fq.gz'\n",
    "\n",
    "# Set # of Processors to Use\n",
    "processors = 20\n",
    "\n",
    "# Classification Database to Use \n",
    "# options: \"Silva\" [default] | \"GreenGenes\" \n",
    "db = \"Silva\"   \n",
    "\n",
    "## Enter Minimum Support for Keeping QIIME Classification\n",
    "# Note: Classifications that do not meet this criteria will simply be retained, but labeled 'putative'\n",
    "min_support = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Concatenate Barcodes for QIIME2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Note: QIIME takes a single barcode file. The command 'extract_barcodes.py' concatenates the forward and reverse read barcode and attributes it to a single read.\n",
    "\n",
    "# See http://qiime.org/tutorials/processing_illumina_data.html\n",
    "\n",
    "for dataset in datasets:\n",
    "    directory = dataset[1]\n",
    "    index1 = directory+indexFile1\n",
    "    index2 = directory+indexFile2\n",
    "    \n",
    "    # Run extract_barcodes to merge the two index files\n",
    "    !python2 /opt/anaconda2/bin/extract_barcodes.py --input_type barcode_paired_end -f $index1 -r $index2 --bc1_len 8 --bc2_len 8 -o $directory/output\n",
    "\n",
    "    # QIIME2 import requires a directory containing files names: forward.fastq.gz, reverse.fastq.gz and barcodes.fastq.gz \n",
    "    !ln -s $directory$readFile1 $directory/output/forward.fastq.gz\n",
    "    !ln -s $directory$readFile2 $directory/output/reverse.fastq.gz\n",
    "    \n",
    "    # Gzip the barcodes files (apparently necessary)\n",
    "    !pigz -p 5 $directory/output/barcodes.fastq\n",
    "\n",
    "    # Removed orphaned reads files (not needed)\n",
    "    !rm $directory/output/reads?.fastq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Import into QIIME2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime tools import\",\n",
    "        \"--type EMPPairedEndSequences\",\n",
    "        \"--input-path \"+directory+\"output/\",\n",
    "        \"--output-path \"+directory+\"output/\"+name+\".qza\"\n",
    "    ]))\n",
    "    \n",
    "    # This more direct command is broken by the fact QIIME uses multiple dashes in their arguments (is my theory)\n",
    "    #!qiime tools import --type EMPPairedEndSequences --input-path $directory/output --output-path $directory/output/$name.qza\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Demultiplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########\n",
    "## Note: The barcode you supply to QIIME is now a concatenation of your forward and reverse barcode.\n",
    "# Your 'forward' barcode is actually the reverse complement of your reverse barcode and the 'reverse' is your forward barcode. The file 'primers.complete.csv' provides this information corresponding to the Buckley Lab 'primer number'\n",
    "# This quirk could be corrected in how different sequencing facilities pre-process the output from the sequencer\n",
    "\n",
    "##\n",
    "## SLOW STEP (~ 2 - 4 hrs)\n",
    "##\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    metadata = dataset[2]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime demux emp-paired\",\n",
    "        \"--m-barcodes-file \"+directory+metadata,\n",
    "        \"--m-barcodes-column BarcodeSequence\",\n",
    "        \"--i-seqs \"+directory+\"output/\"+name+\".qza\",\n",
    "        \"--o-per-sample-sequences \"+directory+\"output/\"+name+\".demux\"\n",
    "    ]))\n",
    "    \n",
    "    # This more direct command is broken by the fact QIIME uses multiple dashes in their arguments (is my theory)\n",
    "    #!qiime demux emp-paired --m-barcodes-file $directory/$metadata --m-barcodes-category BarcodeSequence --i-seqs $directory/output/$name.qza --o-per-sample-sequences $directory/output/$name.demux\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Visualize Quality Scores and Determine Trimming Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Based on the Graph Produced using the Following Command enter the trim and truncate values. Trim refers to the start of a sequence and truncate the total length (i.e. number of bases to remove from end)\n",
    "\n",
    "# The example in the Atacam Desert Tutorial trims 13 bp from the start of each read and does not remove any bases from the end of the 150 bp reads:\n",
    "#  --p-trim-left-f 13 \\  \n",
    "#  --p-trim-left-r 13 \\\n",
    "#  --p-trunc-len-f 150 \\\n",
    "#  --p-trunc-len-r 150\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime demux summarize\",\n",
    "        \"--i-data \"+directory+\"/output/\"+name+\".demux.qza\",\n",
    "        \"--o-visualization \"+directory+\"/output/\"+name+\".demux.QC.summary.qzv\"\n",
    "    ]))\n",
    "    \n",
    "    ## Take the output from this command and drop it into:\n",
    "    #https://view.qiime2.org\n",
    "\n",
    "wait_for_user = input(\"The script will now wait for you to input trimming parameters in the next cell. You will need to take the .qzv files for each library and visualize them at <https://view.qiime2.org>. This is hopefully temporary, while QIIME2 developers improve on q2view.\\n\\n[ENTER ANYTHING. THIS IS ONLY MEANT TO PAUSE THE PIPELINE]\")\n",
    "print(\"\\nThe script is now proceeding. Stay tuned to make sure trimming works.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Trimming Parameters | USER INPUT REQUIRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## User Input Required\n",
    "trim_dict = {}\n",
    "\n",
    "## Input your trimming parameters into a python dictionary for all libraries\n",
    "#trim_dict[\"LibraryName1\"] = [trim_forward, truncate_forward, trim_reverse, truncate_reverse]\n",
    "#trim_dict[\"LibraryName2\"] = [trim_forward, truncate_forward, trim_reverse, truncate_reverse]\n",
    "\n",
    "## Example\n",
    "trim_dict[\"bioblitz\"] = [1, 240, 1, 190]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Trim, Denoise and Join (aka 'Merge') Reads Using DADA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## SLOW STEP (~ 6 - 8 hrs, IF multithreading is used)\n",
    "##\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime dada2 denoise-paired\",\n",
    "        \"--i-demultiplexed-seqs \"+directory+\"/output/\"+name+\".demux.qza\",\n",
    "        \"--o-table \"+directory+\"/output/\"+name+\".table\",\n",
    "        \"--o-representative-sequences \"+directory+\"/output/\"+name+\".rep.seqs.final\",\n",
    "        \"--p-trim-left-f \"+str(trim_dict[name][0]),\n",
    "        \"--p-trim-left-r \"+str(trim_dict[name][2]),\n",
    "        \"--p-trunc-len-f \"+str(trim_dict[name][1]),\n",
    "        \"--p-trunc-len-r \"+str(trim_dict[name][3]),\n",
    "        \"--p-n-threads\",\n",
    "        processors\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Create Summary of OTUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    metadata = dataset[2]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime feature-table summarize\",\n",
    "        \"--i-table \"+directory+\"/output/\"+name+\".table.qza\",\n",
    "        \"--o-visualization \"+directory+\"/output/\"+name+\".table.qzv\",\n",
    "        \"--m-sample-metadata-file \"+directory+metadata\n",
    "    ]))\n",
    "\n",
    "    os.system(' '.join([\n",
    "        \"qiime feature-table tabulate-seqs\",\n",
    "        \"--i-data \"+directory+\"/output/\"+name+\".rep.seqs.final.qza\",\n",
    "        \"--o-visualization \"+directory+\"/output/\"+name+\".rep.seqs.final.qzv\"\n",
    "    ])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Make Phylogenetic Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    metadata = dataset[2]\n",
    "    domain = dataset[3]\n",
    "\n",
    "    if domain != \"fungi\":\n",
    "        # Generate Alignment with MAFFT\n",
    "        os.system(' '.join([\n",
    "            \"qiime alignment mafft\",\n",
    "            \"--i-sequences \"+directory+\"/output/\"+name+\".rep.seqs.final.qza\",\n",
    "            \"--o-alignment \"+directory+\"/output/\"+name+\".rep.seqs.aligned.qza\",\n",
    "            \"--p-n-threads\",\n",
    "            processors\n",
    "        ]))\n",
    "\n",
    "        # Mask Hypervariable parts of Alignment\n",
    "        os.system(' '.join([\n",
    "            \"qiime alignment mask\",\n",
    "            \"--i-alignment \"+directory+\"/output/\"+name+\".rep.seqs.aligned.qza\",\n",
    "            \"--o-masked-alignment \"+directory+\"/output/\"+name+\".rep.seqs.aligned.masked.qza\"\n",
    "        ])) \n",
    "\n",
    "        # Generate Tree with FastTree\n",
    "        os.system(' '.join([\n",
    "            \"qiime phylogeny fasttree\",\n",
    "            \"--i-alignment \"+directory+\"/output/\"+name+\".rep.seqs.aligned.masked.qza\",\n",
    "            \"--o-tree \"+directory+\"/output/\"+name+\".rep.seqs.tree.unrooted.qza\",\n",
    "            \"--p-n-threads\",\n",
    "            processors\n",
    "        ])) \n",
    "\n",
    "        # Root Tree\n",
    "        os.system(' '.join([\n",
    "            \"qiime phylogeny midpoint-root\",\n",
    "            \"--i-tree \"+directory+\"/output/\"+name+\".rep.seqs.tree.unrooted.qza\",\n",
    "            \"--o-rooted-tree \"+directory+\"/output/\"+name+\".rep.seqs.tree.final.qza\"\n",
    "        ])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Classify Seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example of creating Silva Classifier DB  (very slow: use a pre-built one if possible)\n",
    "\n",
    "#### For Silva, remove all the alignment information (unsure of the impact of keeping it) using the following python code:\n",
    "```python \n",
    "import re\n",
    "from Bio import SeqIO\n",
    "\n",
    "output = open(\"silva.nr_v128.fasta\", \"w\")\n",
    "\n",
    "for record in SeqIO.parse(open(\"silva.nr_v128.align\", \"rU\"), \"fasta\") :\n",
    "    seq = str(record.seq)\n",
    "    seq = re.sub(\"\\.|-\",\"\",seq)  # Remove \".\" and \"-\"\n",
    "\n",
    "    output.write(\">\"+record.id+\"\\n\"+seq+\"\\n\")```\n",
    "\n",
    "#### Import fasta sequence file and taxonomy file as .qza\n",
    "```bash\n",
    "qiime tools import\n",
    "  --type 'FeatureData[Sequence]'\n",
    "  --input-path silva.nr_v128.fasta\n",
    "  --output-path silva.nr_v128.qza\n",
    "\n",
    "qiime tools import \n",
    "  --type 'FeatureData[Taxonomy]' \n",
    "  --source-format HeaderlessTSVTaxonomyFormat \n",
    "  --input-path silva.nr_v128.tax \n",
    "  --output-path silva.nr_v128.taxonomy.qza```\n",
    "\n",
    "#### Run QIIME2 'fit-classifier-naive-bayes'\n",
    "```bash\n",
    "qiime feature-classifier fit-classifier-naive-bayes \n",
    "  --i-reference-reads silva.nr_v128.qza \n",
    "  --i-reference-taxonomy silva.nr_v128.taxonomy.qza \n",
    "  --o-classifier silva.nr_v128.nb.classifier.qza```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Note: Different QIIME2 versions can conflict with previously donwloaded databases. This section might have to be updated.\n",
    "try:\n",
    "    if db == \"GreenGenes\":\n",
    "        classification_db = \"/home/db/GreenGenes/qiime2_13.8.99_515.806_nb.classifier.qza\"\n",
    "    else:\n",
    "        classification_db = \"/home/db/Silva/silva.nr_v128.nb.classifier.qza\"\n",
    "        \n",
    "except:\n",
    "        classification_db = \"/home/db/Silva/silva.nr_v128.nb.classifier.qza\"\n",
    "        \n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    metadata = dataset[2]\n",
    "    domain = dataset[3]\n",
    "\n",
    "    # Classify\n",
    "    if domain == 'bacteria':\n",
    "        os.system(' '.join([\n",
    "            \"qiime feature-classifier classify-sklearn\",\n",
    "            \"--i-classifier\",\n",
    "            classification_db,\n",
    "            \"--i-reads \"+directory+\"/output/\"+name+\".rep.seqs.final.qza\",\n",
    "            \"--o-classification \"+directory+\"/output/\"+name+\".taxonomy.final.qza\",\n",
    "            \"--p-n-jobs\",\n",
    "            processors\n",
    "        ]))\n",
    "\n",
    "    if domain == 'fungi':\n",
    "        os.system(' '.join([\n",
    "            \"qiime feature-classifier classify-sklearn\",\n",
    "            \"--i-classifier /home/db/UNITE/qiime2_unite_ver7.99_20.11.2016_classifier.qza\",\n",
    "            \"--i-reads \"+directory+\"/output/\"+name+\".rep.seqs.final.qza\",\n",
    "            \"--o-classification \"+directory+\"/output/\"+name+\".taxonomy.final.qza\",\n",
    "            \"--p-n-jobs\",\n",
    "            processors\n",
    "        ]))\n",
    "\n",
    "    # Output Summary\n",
    "    os.system(' '.join([\n",
    "        \"qiime metadata tabulate\",\n",
    "        \"--m-input-file \"+directory+\"/output/\"+name+\".taxonomy.final.qza\",\n",
    "        \"--o-visualization \"+directory+\"/output/\"+name+\".taxonomy.final.summary.qzv\"\n",
    "    ])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11: Prepare Data for Import to Phyloseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make Function to Re-Format Taxonomy File to Contain Full Column Information \n",
    "# and factor in the certain of the taxonomic assignment\n",
    "\n",
    "def format_taxonomy(tax_file, classification_db, min_support):\n",
    "    output = open(re.sub(\".tsv\",\".fixed.tsv\",tax_file), \"w\")\n",
    "\n",
    " \n",
    "    # Silva db lacks species classifications\n",
    "    if classification_db == \"GreenGenes\":\n",
    "        full_rank_length = 7\n",
    "        output.write(\"\\t\".join([\"OTU\",\"Domain\",\"Phylum\",\"Class\",\"Order\",\"Family\",\"Genus\",\"Species\"])+\"\\n\")\n",
    "    else:\n",
    "        full_rank_length = 6  \n",
    "        output.write(\"\\t\".join([\"OTU\",\"Domain\",\"Phylum\",\"Class\",\"Order\",\"Family\",\"Genus\"])+\"\\n\")\n",
    "        \n",
    "    with open(tax_file, \"r\") as f:\n",
    "        next(f) #skip header\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            line = line.split(\"\\t\")\n",
    "\n",
    "            read_id = line[0]\n",
    "            tax_string = line[1]\n",
    "\n",
    "            ## Remove All Underscore Garbage (I need aesthetics)\n",
    "            if classification_db == \"GreenGenes\":\n",
    "                tax_string = re.sub(\"k__|p__|c__|o__|f__|g__|s__\",\"\",tax_string)\n",
    "            else:\n",
    "                tax_string = re.sub(\"_cl|_or|_fa|_ge\",\"\",tax_string)\n",
    "            \n",
    "            # Split full rank into ranks\n",
    "            full_rank = tax_string.split(\";\")\n",
    "          \n",
    "            # Getting trailing empty tab in Silva\n",
    "            if full_rank[len(full_rank)-1] == \"\":\n",
    "                    full_rank = full_rank[:-1]\n",
    "                    \n",
    "            ## Identify the Lowest Classified Taxonomic Rank\n",
    "            # Account for cases when a taxonomic rank contains an empty space (common in GreenGenes output)\n",
    "            last_classified = full_rank[len(full_rank)-1]            \n",
    "\n",
    "            count = 1\n",
    "            while last_classified == \" \":\n",
    "                last_classified = full_rank[len(full_rank)-count]\n",
    "                count = count + 1\n",
    "\n",
    "            # Annotate the last classified as 'putative' if it does not meet the minimum support criteria\n",
    "            # Older versions of this script contain code to designate all taxonomic ranks as 'putative' in this case, but \n",
    "            # this seems conservative\n",
    "            if float(line[2]) < float(min_support):\n",
    "                    full_rank[full_rank.index(last_classified)] = \"putative \"+last_classified\n",
    "                    last_classified = \"putative \"+last_classified\n",
    "                    \n",
    "            # Add in columns containing unclassified taxonomic information\n",
    "            try: # In Silva, many classifications are a single entry (which breaks from the reliance on lists for full_rank.index)\n",
    "                for n in range(full_rank.index(last_classified)+1, full_rank_length, 1):               \n",
    "                    try:\n",
    "                        full_rank[n] = \"unclassified \"+last_classified\n",
    "                    except:\n",
    "                        full_rank.append(\"unclassified \"+last_classified)\n",
    "            except:\n",
    "                for n in range(0, full_rank_length, 1):               \n",
    "                    try:\n",
    "                        full_rank[n] = \"unclassified \"+last_classified\n",
    "                    except:\n",
    "                        full_rank.append(\"unclassified \"+last_classified)\n",
    "                    \n",
    "            # Clean-up the trailing whitespace introduced in Silva classification \n",
    "            if classification_db == \"Silva\":\n",
    "                full_rank = [x.strip(' ') for x in full_rank]\n",
    "\n",
    "            # Write Taxonomy to File\n",
    "            output.write(read_id+\"\\t\"+'\\t'.join(full_rank)+\"\\n\")\n",
    "            \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "## Export from QIIME2\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    metadata = dataset[2]\n",
    "    domain = dataset[3]\n",
    "\n",
    "    ## Final Output Names\n",
    "    fasta_file = directory+\"/output/\"+name+\".rep.seqs.final.fasta\"\n",
    "    tree_file = directory+\"/output/\"+name+\".tree.final.nwk\"\n",
    "    tax_file = directory+\"/output/\"+name+\".taxonomy.final.tsv\"\n",
    "    count_table = directory+\"/output/\"+name+\".counts.final.biom\"\n",
    "\n",
    "    # Export Classifications\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        directory+\"/output/\"+name+\".taxonomy.final.qza\",\n",
    "        \"--output-dir \"+directory+\"/output/\"\n",
    "    ]))\n",
    "    \n",
    "    # Reformat Classifications to meet phyloseq format   \n",
    "    format_taxonomy(directory+\"/output/taxonomy.tsv\", db, min_support)\n",
    "\n",
    "    # Export SV Table\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        directory+\"/output/\"+name+\".table.qza\",\n",
    "        \"--output-dir \"+directory+\"/output/\"\n",
    "    ]))\n",
    "\n",
    "    # Export SV Sequences\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        directory+\"/output/\"+name+\".rep.seqs.final.qza\",\n",
    "        \"--output-dir \"+directory+\"/output/\"\n",
    "    ]))\n",
    "    \n",
    "    # Export Tree\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        directory+\"/output/\"+name+\".rep.seqs.tree.final.qza\",\n",
    "        \"--output-dir \"+directory+\"/output/\"\n",
    "    ]))\n",
    "    \n",
    "    # Rename Exported Files\n",
    "    %mv $directory/output/dna-sequences.fasta $fasta_file\n",
    "    %mv $directory/output/feature-table.biom $count_table\n",
    "    %mv $directory/output/taxonomy.fixed.tsv $tax_file\n",
    "    \n",
    "    if domain == \"bacteria\":\n",
    "        %mv $directory/output/tree.nwk $tree_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 13: Get 16S rRNA Gene Copy Number (rrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This step is based on the database contructed for the software 'copyrighter'\n",
    "## The software itself lacked information about datastructure (and, the import of a biom from QIIME2 failed, likely because there are multiple versions of the biom format)\n",
    "downloaded = \"N\"\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    domain = dataset[3]\n",
    "\n",
    "    if domain == 'bacteria':\n",
    "        if downloaded == \"N\":\n",
    "            ## Download copyrighter database\n",
    "            !git clone https://github.com/fangly/AmpliCopyrighter $directory/temp/\n",
    "\n",
    "            ## There are multiple GreenGenes ID numbers for a given taxonomic string.\n",
    "            ## However, the copyrighter database uses the same average rrn copy number.\n",
    "            ## We will therefore just use the taxonomic strings, since QIIME2 does not output the ID numbers\n",
    "\n",
    "            !sed -e '1,1075178d; 1078115d' $directory/temp/data/201210/ssu_img40_gg201210.txt > $directory/output/copyrighter.tax.strings.tsv\n",
    "\n",
    "            ## Create Dictionary of rrnDB\n",
    "            rrnDB = {}\n",
    "\n",
    "            with open(directory+\"/output/copyrighter.tax.strings.tsv\", \"r\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    line = line.split(\"\\t\")\n",
    "\n",
    "                    try:\n",
    "                        rrnDB[line[0]] = line[1]\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            downloaded = \"Y\"\n",
    "            \n",
    "        ## Attribute rrn to readID from taxonomy.tsv\n",
    "        output = open(directory+\"/output/\"+name+\".seqID.to.rrn.final.tsv\",\"w\")\n",
    "        output.write(\"Feature ID\\trrn\\n\")\n",
    "\n",
    "        with open(directory+\"/output/taxonomy.tsv\", \"r\") as f:\n",
    "            missing = 0\n",
    "            total = 0\n",
    "            next(f)  # Skip Header\n",
    "\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "\n",
    "                seqID = line[0]\n",
    "\n",
    "                try:\n",
    "                    rrn = rrnDB[line[1]]\n",
    "\n",
    "                except:\n",
    "                    rrn = \"NA\"\n",
    "                    missing = missing + 1\n",
    "\n",
    "                total = total + 1\n",
    "                output.write(seqID+\"\\t\"+rrn+\"\\n\")\n",
    "\n",
    "        print(\"\\nPercent of OTUs Missing {:.1%}\".format(float(missing)/total))\n",
    "        print(\"Don't Panic! The majority of missing OTUs could be low abundance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 14: Import into Phyloseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup R-Magic for Jupyter Notebooks\n",
    "import rpy2\n",
    "%load_ext rpy2.ipython\n",
    "\n",
    "def fix_biom_conversion(file):\n",
    "    with open(file, 'r') as fin:\n",
    "        data = fin.read().splitlines(True)\n",
    "    with open(file, 'w') as fout:\n",
    "        fout.writelines(data[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%R library(phyloseq)\n",
    "%R library(ape)\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    metadata = dataset[2]\n",
    "    domain = dataset[3]\n",
    " \n",
    "    #### IMPORT DATA to R\n",
    "    ## For '.tsv' files, use Pandas to create a dataframe and then pipe that to R\n",
    "    ## For '.biom' files, first convert using 'biom convert' on the command-line\n",
    "    ## Had problems importing the count table with pandas, opted for using read.table in R\n",
    "    \n",
    "    # Import Taxonomy File\n",
    "    tax_file = pd.read_csv(directory+\"/output/\"+name+\".taxonomy.final.tsv\", sep=\"\\t\")\n",
    "    %R -i tax_file\n",
    "    %R rownames(tax_file) = tax_file$OTU\n",
    "    %R tax_file$OTU <- NULL\n",
    "    %R tax_file <- tax_file[sort(row.names(tax_file)),] #read names must match the count_table\n",
    "\n",
    "    # Import Sample Data\n",
    "    sample_file = pd.read_table(directory+metadata, keep_default_na=False)\n",
    "    %R -i sample_file\n",
    "    %R rownames(sample_file) = sample_file$X.SampleID   \n",
    "    %R sample_file$X.SampleID <- NULL\n",
    "    %R sample_file$LinkerPrimerSequence <- NULL  ## Clean-up some other stuff\n",
    "    \n",
    "    # Import Count Data\n",
    "    os.system(' '.join([\n",
    "        \"biom convert\",\n",
    "        \"-i\",\n",
    "        directory+\"/output/\"+name+\".counts.final.biom\",\n",
    "        \"-o\",\n",
    "        directory+\"/output/\"+name+\".counts.final.tsv\",\n",
    "        \"--to-tsv\"\n",
    "    ]))\n",
    "    \n",
    "    # The biom converter adds a stupid line that messes with the table formatting\n",
    "    fix_biom_conversion(directory+\"/output/\"+name+\".counts.final.tsv\")\n",
    "\n",
    "    # Finally import\n",
    "    count_table = pd.read_csv(directory+\"/output/\"+name+\".counts.final.tsv\", sep=\"\\t\")\n",
    "    %R -i count_table\n",
    "    %R rownames(count_table) = count_table$X.OTU.ID   \n",
    "    %R count_table$X.OTU.ID <- NULL    \n",
    "    %R count_table <- count_table[sort(row.names(count_table)),] #read names must match the tax_table\n",
    "    \n",
    "    # Convert to Phyloseq Objects\n",
    "    %R p_counts = otu_table(count_table, taxa_are_rows = TRUE)    \n",
    "    %R p_samples = sample_data(sample_file)    \n",
    "    %R p_tax = tax_table(tax_file)\n",
    "    %R taxa_names(p_tax) <- rownames(tax_file) # phyloseq throws out rownames\n",
    "    %R colnames(p_tax) <- colnames(tax_file) # phyloseq throws out colnames\n",
    "    \n",
    "    # Merge Phyloseq Objects\n",
    "    %R p = phyloseq(p_counts, p_tax)\n",
    "\n",
    "    # Import Phylogenetic Tree\n",
    "    if domain == \"bacteria\":\n",
    "        tree_file = directory+\"/output/\"+name+\".tree.final.nwk\"\n",
    "        %R -i tree_file  \n",
    "        %R p_tree <- read.tree(tree_file)\n",
    "    \n",
    "        # Combine All Objects into One Phyloseq\n",
    "        %R p_final <- merge_phyloseq(p, p_samples, p_tree)\n",
    "    \n",
    "    else:\n",
    "        # Combine All Objects into One Phyloseq\n",
    "        %R p_final <- merge_phyloseq(p, p_samples)\n",
    "        \n",
    "    # Save Phyloseq Object as '.rds'\n",
    "    output = directory+\"/output/p_\"+name+\".final.rds\"\n",
    "    %R -i output\n",
    "    %R saveRDS(p_final, file = output)\n",
    "    \n",
    "    # Confirm Output\n",
    "    %R print(p_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 15: Clean-up Intermediate Files and Final Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    directory = dataset[1]\n",
    "    metadata = dataset[2]\n",
    "    \n",
    "    # Remove Files\n",
    "    if domain == \"bacteria\":\n",
    "        %rm -r $directory/output/*tree.unrooted.qza \n",
    "        %rm -r $directory/output/*aligned.masked.qza \n",
    "        \n",
    "    %rm $directory/output/*.biom \n",
    "    #%rm -r $directory/temp/   ## NEED SUDO ACCESS FOR THIS (not sure why)\n",
    "    %rm $directory/output/*barcodes.fastq.gz \n",
    "    %rm $directory/output/taxonomy.tsv\n",
    "    %rm $directory/output/forward.fastq.gz # Just the symlink\n",
    "    %rm $directory/output/reverse.fastq.gz # Just the symlink\n",
    "    %rm $directory/output/copyrighter.tax.strings.tsv\n",
    "    \n",
    "    # Separate Final Files\n",
    "    %mkdir $directory/final/    \n",
    "    %mv $directory/output/*.final.rds $directory/final/\n",
    "    %mv $directory/output/*.taxonomy.final.tsv $directory/final/    \n",
    "    %mv $directory/output/*.counts.final.tsv $directory/final/\n",
    "    %mv $directory/output/*.final.fasta $directory/final/\n",
    "    %cp $directory$metadata $directory/final/\n",
    "    %mv $directory/output/*.seqID.to.rrn.final.tsv $directory/final/ \n",
    "    %mv $directory/output/*.nwk $directory/final/ \n",
    "    \n",
    "    # Gzip and Move Intermediate Files\n",
    "    !pigz -p 10 $directory/output/*.qza\n",
    "    !pigz -p 10 $directory/output/*.qzv\n",
    "    \n",
    "    %mv $directory/output/ $directory/intermediate_files\n",
    "\n",
    "print(\"Your sequences have been successfully saved to 'final' and 'intermediate_files'\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": true,
  "kernelspec": {
   "display_name": "Environment (conda_qiime2-2018.2)",
   "language": "python",
   "name": "conda_qiime2-2018.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
